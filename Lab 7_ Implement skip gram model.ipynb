{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP83ve9tfDaBsXnYgus03Pq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2D-KNc7UjUob","executionInfo":{"status":"ok","timestamp":1715797244385,"user_tz":-330,"elapsed":39753,"user":{"displayName":"PIYUSH AGARWAL (RA2111047010152)","userId":"10536602745476787933"}},"outputId":"3765b753-32fd-4d46-87af-0897ba9ab154"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 32.6065\n","Epoch [200/1000], Loss: 32.1394\n","Epoch [300/1000], Loss: 32.0085\n","Epoch [400/1000], Loss: 31.9492\n","Epoch [500/1000], Loss: 31.9162\n","Epoch [600/1000], Loss: 31.8955\n","Epoch [700/1000], Loss: 31.8813\n","Epoch [800/1000], Loss: 31.8710\n","Epoch [900/1000], Loss: 31.8633\n","Epoch [1000/1000], Loss: 31.8573\n","Word: like, Embedding: [-1.0592855  -0.5826045  -0.44394037  0.8896508   1.6856664   0.39855832\n","  0.5809075  -0.18489188  0.7459566  -0.06274474  1.9815489   0.58402336\n"," -0.70412534  1.2052749  -0.3771717  -1.2036979   0.3987278  -0.6125249\n"," -0.70824     0.5074935   0.33631265 -0.24587905  1.1995177   0.515077\n"," -2.1858046  -2.0980644   0.31460714  0.98080605  0.0961078   1.2869277\n"," -2.3101315  -1.2965968   1.9134912   0.88429177  1.6794944  -0.22442059\n","  2.013761    3.0812306   0.4509621   0.5705621  -0.63860446 -0.6917429\n","  1.0145663   0.29554993  1.7493105  -0.81911665 -1.3683809  -0.24847715\n","  2.2202601  -1.9296569 ]\n","Word: I, Embedding: [ 1.0097629   0.2468085  -0.7215105   1.7959858   1.1447767  -1.1874686\n","  0.7527509   0.6813999   1.9002373   0.23773517  0.3272847  -1.5387607\n","  0.09209424 -0.6270766   0.7723051  -0.4372775  -0.49615678 -2.2735932\n","  0.6621316   1.4074466  -0.9195598  -1.6575592  -0.7445001   0.38241133\n","  1.000773   -0.7354896   1.0289303  -0.04634836  0.3053204  -0.76334316\n","  0.67488986  2.8334327   0.8568227   0.6233679   0.5438993  -1.1740931\n"," -0.42635942  1.9463413   0.00714577 -0.47458082 -2.1334467   0.6127706\n","  1.0000954   2.7140665  -0.5771024  -0.74995613 -0.76961875 -1.376716\n","  0.6186919   0.95436543]\n","Word: Natural, Embedding: [-0.9225515   0.7413874  -0.24871877  0.27425545  0.3131668   0.24703582\n"," -0.4550425   0.13674545 -1.19749    -0.06692524  0.2629957   0.841349\n"," -0.39785478  1.1902635   0.8402922   0.42809063 -0.867032   -0.45881552\n","  0.47694007  1.3909043  -0.5544575   0.11983141  0.8649902   1.4865292\n","  1.0931841   1.6662073   1.8590044  -0.14785993  0.24790029  0.76804906\n","  0.13684824 -2.0469003  -0.9123026  -1.3991708   0.06310305 -1.9168696\n","  0.5734145   0.6675197  -2.5702693   2.4336252   0.04482829 -1.8091604\n"," -0.44647926  1.5132204   0.1132778  -0.49177286 -0.5822169   0.0263261\n","  1.1168005  -0.7432603 ]\n","Word: and, Embedding: [-0.4475376   0.19963564  0.5287464   1.026313   -0.55904704  0.5320324\n"," -2.7186546   0.15769106  0.28636873 -0.11132123 -1.4579138   1.0961319\n"," -0.43174392  0.87833947  0.32810476 -1.4170521   1.0454925   0.6615682\n"," -0.8606182   1.0497153   0.08966159 -1.3424398   0.43822092  0.10907984\n"," -0.9309979   1.0990433   0.8243989  -0.5395806  -1.3541629   2.0920737\n","  0.24171984  0.5332563   0.994846   -0.4569856  -0.57931125  1.4901471\n","  0.29555538  0.29540622 -0.17075521 -0.12608233  0.05479097  2.0761814\n"," -0.47118592 -1.6071569  -1.0747187   0.03192275  0.42782745  1.455967\n","  0.17932864  0.3768937 ]\n","Word: Networks, Embedding: [ 1.7697585  -2.1864667   0.99746144 -1.4110743   2.1242032   2.8840427\n","  0.08269712 -0.05934742 -0.36173695 -0.54481447 -0.21336658  0.2596923\n","  0.31031036 -0.4421599  -1.8274149   0.55327237 -0.0489593  -0.3518731\n","  1.5378344  -0.3312842   0.9890245  -0.02230128  0.13954788 -0.14709935\n"," -0.09819537  0.6256272  -0.8332266   0.6124937   0.44104186  0.70506245\n","  3.602676    0.18983667  1.4705116   0.32859915  1.2866272   0.18718414\n"," -2.3430274  -0.45724112 -2.356948   -0.26651263  0.36025518  1.3115559\n","  0.0830686  -1.4656801  -1.504952    0.08276804  1.208075   -0.7953155\n"," -1.4141979   1.7281061 ]\n","Word: Neural, Embedding: [-1.109047   -0.6151594  -2.9643931  -0.67422646 -0.8726748  -1.8468606\n","  1.1024272  -1.4462476   1.2245501   0.12399293  0.38180563  1.701416\n","  0.94179946  0.5935006   0.84426075  1.3055093   1.9561347  -1.0367699\n"," -1.7257488   0.68006915  1.389633    0.73880935  2.3360388  -0.6032321\n","  0.9804394  -1.5678911   1.1962596   0.51988095 -0.5914624  -1.1130458\n","  0.35928974  0.9516234  -0.56878376  0.07213978  0.93926775  1.0576205\n","  1.9030203  -1.387183   -0.07715496 -1.0634153   1.4670198   1.5334722\n"," -0.9775106  -3.355865    1.184853   -0.29510313  0.5169453   1.4838983\n","  0.9338894   1.9265587 ]\n","Word: Processing, Embedding: [-1.3913633  -0.585244    0.80145055 -1.8401449   1.1980205  -0.77484137\n","  0.8536662  -1.5874617   0.3137006   0.878571    0.3354821   0.8276945\n","  0.84169626 -0.80334634  0.4530503  -0.9725974   0.04480534 -0.3863404\n","  1.2724844  -0.40331826 -0.5335821   0.5647691   0.48940971 -1.2125157\n"," -0.47760147 -0.03007353 -0.59339184  2.1229906  -0.8834968   0.33379894\n","  1.0037186  -0.7119074  -0.710534   -1.3250548   1.2641313  -0.9368666\n"," -0.14122008 -0.65560305 -0.17634144  1.0280732  -1.001596    0.1396788\n"," -0.7146565  -2.0460336  -1.4730588   0.52583754  1.3585186  -0.6587225\n"," -0.7974314  -1.08469   ]\n","Word: Language, Embedding: [ 1.1816745   0.3467721   1.7537253   0.8528882   0.10104244  1.0058603\n","  1.1142609   0.9750789   0.69798326 -1.220418   -0.8747188   1.6823878\n","  1.3069581   0.00707304 -0.4626047  -0.0347563  -0.86132926 -0.6600278\n"," -0.5786668  -0.07520737 -1.4735956   0.10044457 -1.679002    0.72600144\n","  1.0237378  -0.45244256  0.31888494 -0.6884731  -0.37312248  0.14563836\n"," -0.06444015 -0.11347482  0.87701523  0.72369915 -0.18859723 -2.0342333\n"," -0.8785557  -0.09939059  1.1218551  -1.0336921   1.1807466   1.1852757\n"," -1.3460039   1.8489363   0.5664772  -2.503428   -0.63727653 -1.4711897\n"," -0.05766058 -1.0603747 ]\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from collections import defaultdict\n","import numpy as np\n","corpus = \"I like Natural Language Processing and Neural Networks\"\n","\n","corpus = corpus.split()\n","word_to_idx = {word: i for i, word in enumerate(set(corpus))}\n","idx_to_word = {i: word for word, i in word_to_idx.items()}\n","vocab_size = len(word_to_idx)\n","def generate_skipgram_data(corpus, window_size):\n","    data = []\n","    for i, target_word in enumerate(corpus):\n","        target_idx = word_to_idx[target_word]\n","        for j in range(i - window_size, i + window_size + 1):\n","            if j != i and 0 <= j < len(corpus):\n","                context_word = corpus[j]\n","                context_idx = word_to_idx[context_word]\n","                data.append((target_idx, context_idx))\n","    return data\n","window_size = 2\n","embedding_dim = 50\n","learning_rate = 0.001\n","epochs = 1000\n","skipgram_data = generate_skipgram_data(corpus, window_size)\n","class SkipGram(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(SkipGram, self).__init__()\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear = nn.Linear(embedding_dim, vocab_size)\n","    def forward(self, target):\n","        target_embeds = self.embeddings(target)\n","        output = self.linear(target_embeds)\n","        return output\n","model = SkipGram(vocab_size, embedding_dim)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","for epoch in range(epochs):\n","    total_loss = 0\n","    for target, context in skipgram_data:\n","        target_var = Variable(torch.LongTensor([target]))\n","        context_var = Variable(torch.LongTensor([context]))\n","        optimizer.zero_grad()\n","        output = model(target_var)\n","        loss = criterion(output, context_var)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.data.item()\n","    if (epoch + 1) % 100 == 0:\n","        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {total_loss:.4f}')\n","word_embeddings = model.embeddings.weight.data.numpy()\n","for i in range(vocab_size):\n","    word = idx_to_word[i]\n","    embedding = word_embeddings[i]\n","    print(f'Word: {word}, Embedding: {embedding}')"]}]}